# ğŸ§® Analyse Factorielle des Correspondances (AFC) â€” Projet Jupyter

## ğŸ“˜ Introduction

Lâ€™**Analyse Factorielle des Correspondances (AFC)** est une mÃ©thode statistique utilisÃ©e pour **analyser un tableau de contingence** (tableau de comptages) et **reprÃ©senter les relations entre les lignes et les colonnes** dans un espace de faible dimension.
Elle est particuliÃ¨rement pertinente pour **lâ€™Ã©tude de donnÃ©es qualitatives**, lorsque lâ€™on souhaite identifier des **profils similaires** ou des **corrÃ©lations entre modalitÃ©s**.

Dans ce projet, nous implÃ©mentons **une AFC complÃ¨te en Python**, Ã©tape par Ã©tape, **sans bibliothÃ¨que spÃ©cialisÃ©e** (comme `prince` ou `factoMineR`), afin de **maÃ®triser les fondements mathÃ©matiques et algorithmiques** de la mÃ©thode.

Les donnÃ©es sont **gÃ©nÃ©rÃ©es artificiellement** pour simuler un tableau de contingence rÃ©aliste :

```python
import numpy as np
import pandas as pd

np.random.seed(42)
n_lignes, n_colonnes = 25, 5
donnees_afc = np.random.poisson(lam=10, size=(n_lignes, n_colonnes))
lignes = [f"Individu_{i+1}" for i in range(n_lignes)]
colonnes = [f"Variable_{chr(65+j)}" for j in range(n_colonnes)]
df_afc = pd.DataFrame(donnees_afc, index=lignes, columns=colonnes)
```

---

## ğŸ¯ Objectif

Lâ€™objectif est de **comprendre et implÃ©menter chaque Ã©tape de lâ€™AFC** :

1. PrÃ©parer les donnÃ©es et calculer les frÃ©quences ;
2. Centrer et pondÃ©rer les donnÃ©es pour obtenir la matrice du khi-deux ;
3. RÃ©aliser une dÃ©composition en valeurs singuliÃ¨res (SVD) ;
4. Extraire les **valeurs propres**, les **coordonnÃ©es factorielles**, et les **contributions** ;
5. Visualiser les rÃ©sultats sur les deux premiers axes factoriels.

---

## âš™ï¸ Ã‰tape 1 â€” Construction du tableau et frÃ©quences

### But :

Transformer le tableau brut ( N ) (effectifs) en un tableau de **frÃ©quences relatives** ( P ), puis calculer les **profils marginaux** des lignes et des colonnes.

### Formules :

[
P = \frac{N}{n}
]
[
r_i = \sum_j P_{ij}, \quad c_j = \sum_i P_{ij}
]

### RÃ´le :

* ( P ) : pondÃ¨re les effectifs pour supprimer lâ€™effet de la taille totale.
* ( r ) et ( c ) : reprÃ©sentent les **poids** (ou masses) des lignes et colonnes, câ€™est-Ã -dire leur importance dans le total.

```python
N = df_afc.values
n_total = N.sum()
P = N / n_total
r = P.sum(axis=1).reshape(-1, 1)
c = P.sum(axis=0).reshape(1, -1)
```

---

## âš™ï¸ Ã‰tape 2 â€” Centrage et pondÃ©ration du tableau

### But :

Extraire la **structure dâ€™association** entre les lignes et les colonnes en Ã©liminant lâ€™effet des marges.

### Formule :

[
S = D_r^{-1/2} (P - r c) D_c^{-1/2}
]
oÃ¹ :

* ( D_r ) et ( D_c ) sont les matrices diagonales contenant respectivement ( r_i ) et ( c_j ) ;
* ( P - rc ) mesure les Ã©carts entre la frÃ©quence observÃ©e et la frÃ©quence thÃ©orique sous indÃ©pendance.

### RÃ´le :

Cette Ã©tape recentre les donnÃ©es autour de lâ€™hypothÃ¨se dâ€™indÃ©pendance et **met toutes les lignes et colonnes sur un pied dâ€™Ã©galitÃ©**.

```python
Dr_inv_sqrt = np.diag(1 / np.sqrt(r.flatten()))
Dc_inv_sqrt = np.diag(1 / np.sqrt(c.flatten()))
S = Dr_inv_sqrt @ (P - r @ c) @ Dc_inv_sqrt
```

---

## âš™ï¸ Ã‰tape 3 â€” DÃ©composition en valeurs singuliÃ¨res (SVD)

### But :

Extraire les **axes factoriels principaux** qui expliquent la variance (inertie) du nuage de points.

### Formule :

[
S = U \Sigma V^T
]
oÃ¹ :

* ( \Sigma ) : matrice diagonale des **valeurs singuliÃ¨res** ;
* ( \lambda_i = \sigma_i^2 ) : **valeurs propres** (inerties).

### RÃ´le :

La SVD permet de **projeter les lignes et colonnes dans un mÃªme espace**.
Chaque axe factoriel correspond Ã  une **dimension latente dâ€™association** entre lignes et colonnes.

```python
U, singular_values, VT = np.linalg.svd(S, full_matrices=False)
eigenvalues = singular_values**2
inertie = 100 * eigenvalues / eigenvalues.sum()
```

---

## âš™ï¸ Ã‰tape 4 â€” CoordonnÃ©es factorielles

### Formules :

[
F = D_r^{-1/2} U \Sigma
]
[
G = D_c^{-1/2} V \Sigma
]

### RÃ´le :

* ( F ) : coordonnÃ©es factorielles des lignes sur les axes principaux.
* ( G ) : coordonnÃ©es factorielles des colonnes sur les mÃªmes axes.

Elles permettent de **reprÃ©senter visuellement** la proximitÃ© entre lignes et colonnes.

```python
V = VT.T
F = Dr_inv_sqrt @ U @ np.diag(singular_values)
G = Dc_inv_sqrt @ V @ np.diag(singular_values)
```

---

## âš™ï¸ Ã‰tape 5 â€” Contributions et qualitÃ©s de reprÃ©sentation

### But :

Ã‰valuer lâ€™importance de chaque point (ligne ou colonne) dans la construction des axes.

### Formules :

[
\text{CTR}*{ij} = \frac{r_i F*{ij}^2}{\lambda_j}
\quad ; \quad
\text{COS2}*{ij} = \frac{F*{ij}^2}{\sum_k F_{ik}^2}
]

### RÃ´le :

* **CTR (contribution)** : indique combien chaque ligne/colonne contribue Ã  un axe ;
* **COSÂ² (qualitÃ© de reprÃ©sentation)** : mesure la qualitÃ© du placement du point sur un axe (analogue Ã  un ( R^2 )).

```python
CTR_rows = (r * (F**2)) / eigenvalues
CTR_rows = CTR_rows / CTR_rows.sum(axis=0)
COS2_rows = (F**2) / F.sum(axis=1, keepdims=True)**2
```

---

## âš™ï¸ Ã‰tape 6 â€” Visualisation du plan factoriel

### But :

Visualiser les relations entre individus (lignes) et variables (colonnes).

### RÃ´le :

* Les **points proches** traduisent des **profils similaires**.
* Les **axes principaux** concentrent lâ€™essentiel de lâ€™information.
* Les **lignes** et **colonnes** peuvent Ãªtre reprÃ©sentÃ©es conjointement.

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(F_df["Axe_1"], F_df["Axe_2"], color='blue', label="Lignes")
plt.scatter(G_df["Axe_1"], G_df["Axe_2"], color='red', marker='x', label="Colonnes")

for i, txt in enumerate(df_afc.index):
    plt.annotate(txt, (F_df["Axe_1"].iloc[i], F_df["Axe_2"].iloc[i]), fontsize=8)

for j, txt in enumerate(df_afc.columns):
    plt.annotate(txt, (G_df["Axe_1"].iloc[j], G_df["Axe_2"].iloc[j]), color='red', fontsize=9)

plt.axhline(0, color='gray', linewidth=0.8)
plt.axvline(0, color='gray', linewidth=0.8)
plt.title("Plan factoriel (Axes 1 et 2) - Analyse Factorielle des Correspondances")
plt.xlabel("Axe 1")
plt.ylabel("Axe 2")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()
```

---

## ğŸ“Š InterprÃ©tation pratique

1. **Valeurs propres (inertie)** : mesurent la part de variance expliquÃ©e par chaque axe.
   â†’ Les deux premiers axes concentrent souvent 60â€“80 % de lâ€™information.
2. **CoordonnÃ©es factorielles** : permettent dâ€™identifier les profils proches ou opposÃ©s.
3. **CTR et COSÂ²** : aident Ã  savoir **quelles lignes ou colonnes** sont les plus importantes sur un axe.
4. **Graphique factoriel** : synthÃ©tise visuellement les associations entre modalitÃ©s.

---

## ğŸ§  Bilan et pertinence de lâ€™AFC

Lâ€™AFC est une mÃ©thode :

* **descriptive** (aucune hypothÃ¨se prÃ©alable) ;
* **exploratoire** (fait Ã©merger des structures cachÃ©es) ;
* **visuelle** (les cartes factoriels offrent une lecture intuitive des corrÃ©lations).

Dans la pratique :

* Elle est utilisÃ©e en **marketing**, **sociologie**, **analyse textuelle**, ou **Ã©tudes dâ€™opinion**.
* Elle permet de **rÃ©sumer un grand tableau de donnÃ©es qualitatives** en quelques axes interprÃ©tables.

---

## ğŸ§© RÃ©fÃ©rences thÃ©oriques

* BenzÃ©cri, J.-P. (1973). *Lâ€™Analyse des DonnÃ©es â€” Tome 2 : Lâ€™Analyse des Correspondances*. Dunod.
* Greenacre, M. (2017). *Correspondence Analysis in Practice*. Chapman & Hall/CRC.
* Saporta, G. (2006). *ProbabilitÃ©s, analyse des donnÃ©es et statistique*. Technip.

---

## ğŸ Conclusion

Ce travail illustre :

* La **traduction directe des formules mathÃ©matiques** en code Python ;
* La **dÃ©marche complÃ¨te dâ€™une AFC**, du tableau brut Ã  lâ€™interprÃ©tation graphique ;
* La **valeur pÃ©dagogique** de reconstruire soi-mÃªme lâ€™algorithme sans dÃ©pendre de bibliothÃ¨ques toutes faites.

Lâ€™implÃ©mentation manuelle dÃ©montre que lâ€™AFC repose sur :

* Une **logique matricielle Ã©lÃ©gante** (centrage, normalisation, SVD) ;
* Une **visualisation intuitive** qui relie thÃ©orie et interprÃ©tation pratique.
